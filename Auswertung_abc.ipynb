{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verkettung von Motiven im Typ und Typencluster ohne d Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:11:21.683424Z",
     "start_time": "2024-01-25T21:11:21.667056Z"
    }
   },
   "outputs": [],
   "source": [
    "mZahl = \"a551\"\n",
    "mWert=\"a551:h:Krankheit\"\n",
    "haeufigkeit = 2\n",
    "def graph_update():\n",
    "    graph.update(alle_aeste(\"\", df))\n",
    "    graph.update(alle_aeste(\"\", df))\n",
    "    graph.update(alle_aeste(\"\", df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:11:22.943093Z",
     "start_time": "2024-01-25T21:11:21.693040Z"
    }
   },
   "outputs": [],
   "source": [
    "maerchen=\"\" \n",
    "import xml.etree.ElementTree as ET\n",
    "root_node = ET.parse('Textkorpus.xml').getroot()\n",
    "never_saved = True\n",
    "for ganze in root_node.findall(\".//{http://www.tei-c.org/ns/1.0}text\"):\n",
    "    if ('deu' in ganze.attrib['{http://www.w3.org/XML/1998/namespace}id']):\n",
    "        #and 'ita' not in ganze.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "        #and 'ru' not in ganze.attrib['{http://www.w3.org/XML/1998/namespace}id']):\n",
    "        #and 'deu' not in ganze.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "        #or 'uuu'  in ganze.attrib['{http://www.w3.org/XML/1998/namespace}id']):\n",
    "        for body in ganze.findall(\".//{http://www.tei-c.org/ns/1.0}body\"):\n",
    "            for absatz in body.findall(\".//{http://www.tei-c.org/ns/1.0}p\"):\n",
    "                for phrase in absatz.findall(\".//{http://www.tei-c.org/ns/1.0}seg\"):\n",
    "                        labela = phrase.attrib['{www.dglab.uni-jena.de/vmf/a}ana']\n",
    "                        labelb1 = phrase.attrib['{www.dglab.uni-jena.de/vmf/b1}ana']\n",
    "                        labelb2 = phrase.attrib['{www.dglab.uni-jena.de/vmf/b2}ana']\n",
    "                        labelb3 = phrase.attrib['{www.dglab.uni-jena.de/vmf/b3}ana']\n",
    "                        labelb4 = phrase.attrib['{www.dglab.uni-jena.de/vmf/b4}ana']\n",
    "                        labelb5 = phrase.attrib['{www.dglab.uni-jena.de/vmf/b5}ana']\n",
    "                        labelc1 = phrase.attrib['{www.dglab.uni-jena.de/vmf/c1}ana']\n",
    "                        labelc2 = phrase.attrib['{www.dglab.uni-jena.de/vmf/c2}ana']\n",
    "                        labelc3 = phrase.attrib['{www.dglab.uni-jena.de/vmf/c3}ana']\n",
    "                        labelc4 = phrase.attrib['{www.dglab.uni-jena.de/vmf/c4}ana']\n",
    "                        labelc5 = phrase.attrib['{www.dglab.uni-jena.de/vmf/c5}ana']\n",
    "                        labeld = phrase.attrib['{www.dglab.uni-jena.de/vmf/d}ana']\n",
    "                        quelle= ganze.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "                        inhalt = (phrase.text.lower().strip().replace('|','').replace(':','').replace(\"ä\",\"ae\")\n",
    "                                  .replace(\"ü\",\"ue\").replace(\"ö\",\"oe\").replace(\"ß\",\"ss\").replace(\",\",\"\")\n",
    "                                  .replace(\"«\",\"\").replace(\"»\",\"\").replace(\".\",\"\").replace('=',' ').replace(\";\",\"\")\n",
    "                                  .replace('\"',\"\").replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\").replace(\",\",\"\")\n",
    "                                  .replace(\"\\t\",\" \").replace(\"'\",\"\").replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \")\n",
    "                                  .replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','').replace(\"    \",\" \")\n",
    "                                  .replace(\"   \",\" \").replace(\"  \",\" \").replace('–','').replace('—','').replace('<','')\n",
    "                                  .replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','').replace('„','').replace('(','')\n",
    "                                  .replace(')','').replace('*','').replace(\"\\n\",\"\\n\")\n",
    "                                  .replace(\"'''\",\"\"))\n",
    "                        if labela.startswith('a') and labela!='N' :\n",
    "                            \n",
    "                            maerchen += quelle+','+labela+','+labelb1+','+labelc1+','+labelb2+','+labelc2+','+labelb3+','+labelc3+','+labelb4+','+labelc4+','+labelb5+','+labelc5+','+inhalt+',0'+'\\n'\n",
    "\n",
    "with open(\"text.csv\", 'w', encoding='utf-8') as f:\n",
    "    f.write('quelle,labela,labelb1,labelc1,labelb2,labelc2,labelb3,labelc3,labelb4,labelc4,labelb5,labelc5,inhalt,index_binar')\n",
    "    f.write('\\n')\n",
    "    f.write(maerchen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd1=\"\"\n",
    "abcd2=\"\"\n",
    "abcd3=\"\"\n",
    "abcd4=\"\"\n",
    "abcd5=\"\"\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "df = pd.read_csv('text.csv',  encoding='utf-8')\n",
    "\n",
    "abcd1 += df.quelle+','+df.labela+':'+df.labelb1+':'+df.labelc1+','+df.inhalt+',0'\n",
    "abcd2 += df.quelle+','+df.labela+':'+df.labelb2+':'+df.labelc2+','+df.inhalt+',0'\n",
    "abcd3 += df.quelle+','+df.labela+':'+df.labelb3+':'+df.labelc3+','+df.inhalt+',0'\n",
    "abcd4 += df.quelle+','+df.labela+':'+df.labelb4+':'+df.labelc4+','+df.inhalt+',0'\n",
    "abcd5 += df.quelle+','+df.labela+':'+df.labelb5+':'+df.labelc5+','+df.inhalt+',0'\n",
    "\n",
    "# maca=':0:'\n",
    "# for t in abcd3:\n",
    "#     if maca not in t:\n",
    "recorded=(abcd1+'\\n'+abcd2+'\\n'+abcd3+'\\n'+abcd4+'\\n'+abcd5+'\\n')\n",
    "with open(\"text_aw.csv\", 'w', encoding='utf-8') as w:\n",
    "    w.write('quelle,index_string,inhalt,index_binar\\n')\n",
    "    w.write('000:ballast,ballast,ballast,0\\n')\n",
    "    w.write(\"\".join(recorded))\n",
    "    w.close()\n",
    "import csv\n",
    "mom=[]\n",
    "with open('text_aw.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f) \n",
    "    for line in reader:\n",
    "        if ':N:' not in line[1]:\n",
    "            mom.append(line)\n",
    "import shutil, os\n",
    "with open ('text_ax.txt', 'wt', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(mom)\n",
    "with open('text_ax.txt', 'r', encoding='utf-8') as file:\n",
    "    filedata = file.read()\n",
    "filedata = (filedata.replace(']\",\"[', \"\\n\").replace('\"[','').replace(']\"','').replace(\" '\",\"\")\n",
    "            .replace(\"'\",\"\").replace('_1_','').replace('_11_','').replace('_111_','')\n",
    "            .replace('_12_','').replace('_112_','').replace('_1111_','')\n",
    "            .replace('_2_','').replace('_1112_',''))\n",
    "with open('text_ay.csv', 'w', encoding='utf-8') as file:\n",
    "    file.write(filedata)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:11:24.190477Z",
     "start_time": "2024-01-25T21:11:24.156617Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('text_ay.csv', encoding='utf-8')\n",
    "\n",
    "from collections import Counter\n",
    "def ep_format(ep_full):\n",
    "    return (ep_full.split(':'))[0]\n",
    "def ep_name_format(ep_full):\n",
    "    sublist = (ep_full)\n",
    "    name = \"\"\n",
    "    for strg in sublist:\n",
    "        name += strg#+'_'\n",
    "    return name\n",
    "def quellenvergleich (df, i1, i2):\n",
    "    return df.quelle[i1]==df.quelle[i2]\n",
    "def ast(gesep, df):\n",
    "    ep_tree = {}\n",
    "    a_liste = []\n",
    "    z_liste = []  \n",
    "    df_len = len(df.index_string)\n",
    "    for i, ep in enumerate(df.index_string):\n",
    "        if gesep == ep:\n",
    "            if (i > 0)&(quellenvergleich(df, i, i-1)):\n",
    "                a = df.index_string[i-1]\n",
    "            else:\n",
    "                a = 'Anf-'+ep_name_format(gesep)\n",
    "            if (i < df_len - 1):\n",
    "                if not (quellenvergleich(df, i, i+1)):\n",
    "                    z = 'End-'+ep_name_format(gesep)\n",
    "                else:        \n",
    "                    z = df.index_string[i+1]\n",
    "            else:\n",
    "                z = 'End-'+ep_name_format(gesep)\n",
    "            a_liste.append(a)\n",
    "            z_liste.append(z)\n",
    "    return {gesep: [Counter(a_liste), Counter(z_liste)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:11:25.557996Z",
     "start_time": "2024-01-25T21:11:25.546033Z"
    }
   },
   "outputs": [],
   "source": [
    "def alle_aeste(gesep, df):\n",
    "    episoden_baeume = {} \n",
    "    ep_list = []\n",
    "    for ep_full in df.index_string:         \n",
    "        ep = ep_format(ep_full)     \n",
    "        if gesep == ep:                      \n",
    "            ep_list.append(ep_full)\n",
    "    for ep in set(ep_list):\n",
    "        episoden_baeume.update(ast(ep,df))\n",
    "    return episoden_baeume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:11:29.915657Z",
     "start_time": "2024-01-25T21:11:29.903072Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = alle_aeste(mZahl, df)\n",
    "graph_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:11:31.249124Z",
     "start_time": "2024-01-25T21:11:30.925952Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "def cutoff_ast_data(wuerzel, graph, nachbar_liste, cutoff = 0):\n",
    "    vor = graph[wuerzel][0]\n",
    "    zurueck = graph[wuerzel][1]\n",
    "    nachbarn = vor.copy()\n",
    "    nachbarn.update(zurueck)\n",
    "    for ast_name, ast_gewicht in sorted(nachbarn.items()):\n",
    "        if (ast_gewicht >= cutoff):\n",
    "             nachbar_liste.append([ep_name_format(wuerzel), \n",
    "                                   ep_name_format(ast_name), ast_gewicht])   \n",
    "def cutoff_graph_data(graph, cutoff = 0):\n",
    "    nachbar_liste = []\n",
    "    for i, (k, v) in enumerate(sorted(graph.items())):\n",
    "        cutoff_ast_data(k, graph, nachbar_liste, cutoff)\n",
    "    neue_nachbar_liste = [[i, nachbar] for i, nachbar in enumerate(nachbar_liste)]\n",
    "    return neue_nachbar_liste\n",
    "def interactive_graph_data(graph_data, loesch_index, gew_dict):\n",
    "    for k, v in gew_dict.items():\n",
    "        graph_data[k][1][2] = v\n",
    "    neue_nachbar_liste = [[i, nachbar[1]] for i, \n",
    "                          nachbar in enumerate(graph_data) if i not in loesch_index]\n",
    "    return neue_nachbar_liste\n",
    "def graph_bauer(graph_data):\n",
    "    G = nx.DiGraph() #DiGraph\n",
    "    w_liste=[]\n",
    "    for el in graph_data:\n",
    "        n1 = el[1][0]\n",
    "        n2 = el[1][1]\n",
    "        w = el[1][2]\n",
    "        w_liste.append(w)\n",
    "        G.add_edge(n1, n2, weight=w, title=w)\n",
    "        G.add_edge(n1, n2, weight=w, title=w)\n",
    "        G.add_node(n1, title=n1, size=15)\n",
    "        G.add_node(n2, title=n2, size=15)\n",
    "\n",
    "    return G,w_liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:11:32.130557Z",
     "start_time": "2024-01-25T21:11:32.122432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto_graph=cutoff_graph_data(graph, haeufigkeit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:12:53.897428Z",
     "start_time": "2024-01-25T21:12:53.677815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "G,W = graph_bauer(auto_graph)\n",
    "    #show_graph(G,W)\n",
    "nt = Network(notebook=True, height='900px', width='100%', directed=False, \n",
    "             bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local')\n",
    "    #nt.from_nx(G)\n",
    "    #nt.show('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T20:53:45.643261Z",
     "start_time": "2024-01-25T20:53:45.629312Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "def neighbour_is_includable(node, neighbour):\n",
    "    designation = \":\".join(node.split(\":\")[0:1])\n",
    "    return neighbour.startswith(designation)\n",
    "\n",
    "def pruned_edges(node, neighbours):\n",
    "    return [neighbor for neighbor in neighbours if neighbour_is_includable(node, neighbor)]\n",
    "\n",
    "def make_pruned_graph(graph):\n",
    "    pruned_graph = {}\n",
    "    for node, (incoming, outgoing) in graph.items():\n",
    "        pruned_incoming = pruned_edges(node, incoming)\n",
    "        pruned_outgoing = pruned_edges(node, outgoing)\n",
    "        pruned_graph[node] = (pruned_incoming, pruned_outgoing)\n",
    "    return pruned_graph\n",
    "\n",
    "def make_backbone_di_graph(pruned_graph):\n",
    "    new_DiGraph = nx.DiGraph()\n",
    "    for node, (incoming, outgoing) in pruned_graph.items():\n",
    "        for in_node in incoming:\n",
    "            new_DiGraph.add_edge(in_node, node)\n",
    "        for out_node in outgoing:\n",
    "            new_DiGraph.add_edge(node, out_node)\n",
    "    return new_DiGraph\n",
    "\n",
    "def signed_shortest_path_length(G, source, target):\n",
    "    try:\n",
    "        pl = nx.dijkstra_path_length(G, source=source, target=target)\n",
    "    except nx.NodeNotFound:        #\n",
    "        pl = np.Inf                #\n",
    "    except nx.NetworkXNoPath:\n",
    "        try:\n",
    "            pl = - nx.dijkstra_path_length(G, source=target, target=source)\n",
    "        except nx.NodeNotFound:   #\n",
    "            pl = np.Inf           #\n",
    "        except nx.NetworkXNoPath:\n",
    "            pl = np.Inf\n",
    "    return pl\n",
    "\n",
    "def create_main_node_distances(start_node, pruned_graph, graph, nx_graph):\n",
    "    distances = { key:[] for key in graph}\n",
    "    backbone_di_graph= make_backbone_di_graph(pruned_graph)\n",
    "    for node in pruned_graph:\n",
    "        distances[node] = {signed_shortest_path_length(backbone_di_graph, start_node, node)}\n",
    "    return distances\n",
    "\n",
    "def add_sub_node_distances(node, sub_nodes, pruned_graph, distances, d):\n",
    "    for sub_node in sub_nodes:\n",
    "        if sub_node not in pruned_graph.keys():\n",
    "            if sub_node not in distances:\n",
    "                distances[sub_node] = set()\n",
    "            new_dists = {dist + d for dist in distances[node]}\n",
    "            distances[sub_node].update(new_dists)\n",
    "\n",
    "def create_distance_dict(start_node, graph, G):\n",
    "    pruned_graph = make_pruned_graph(graph)\n",
    "    distances = create_main_node_distances(start_node, pruned_graph, graph, G)\n",
    "    for node, (incoming, outgoing) in graph.items():\n",
    "        add_sub_node_distances(node, incoming, pruned_graph, distances, -1)\n",
    "        add_sub_node_distances(node, outgoing, pruned_graph, distances, 1)\n",
    "    return distances\n",
    "\n",
    "\n",
    "start_node = mWert\n",
    "distances = create_distance_dict(start_node, graph, G)\n",
    "#print(distances)\n",
    "sorted_keys = sorted(distances, key=lambda x: min(distances[x]))\n",
    "sorted_distances = {key: distances[key] for key in sorted_keys}\n",
    "#print(sorted_distances)\n",
    "msia=\"\"\n",
    "tai = []\n",
    "for k, v in sorted_distances.items():\n",
    "    sia=(f\"{k} {v}\")\n",
    "    msia+=sia+'<br>'\n",
    "    tai.append(sia)\n",
    "    #print(sia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zur CSV-Datei\n",
    "csv_file_path = 'output_1.csv'\n",
    "\n",
    "# Schreibe die Daten in die CSV-Datei\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    # Schreibe die Überschrift (Header) in die CSV-Datei\n",
    "    writer.writerow(['Data'])\n",
    "\n",
    "    # Schreibe die Zeilen in die CSV-Datei\n",
    "    for data in tai:\n",
    "        writer.writerow([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_set_of_chosen_vertives(graph_edge_data):\n",
    "    chosen_vertices=set()\n",
    "    for sub_list in graph_edge_data:\n",
    "        sub_sub_list = sub_list[1]\n",
    "        chosen_vertices.add(sub_sub_list[0])\n",
    "        chosen_vertices.add(sub_sub_list[1])\n",
    "    return chosen_vertices\n",
    "dat=[]\n",
    "graph_data = cutoff_graph_data(graph, haeufigkeit)\n",
    "chosen_vertices=list(make_set_of_chosen_vertives(graph_data))\n",
    "chosen_vertices.sort()    \n",
    "for vertex_name in chosen_vertices:\n",
    "    #if not (vertex_name.startswith(\"End\") or vertex_name.startswith(\"Anf\")or vertex_name.startswith(\"eUNDF\")):\n",
    "        #print(vertex_name)\n",
    "        dat.append(vertex_name)\n",
    "csv_file_path = 'output_2.csv'\n",
    "# Schreibe die Daten in die CSV-Datei\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')    \n",
    "    # Schreibe die Überschrift (Header) in die CSV-Datei\n",
    "    writer.writerow(['Data'])\n",
    "    # Schreibe die Zeilen in die CSV-Datei\n",
    "    for data in dat:\n",
    "        writer.writerow([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Anf-a551:h:Krankheit {-1}\n",
      "a551:h:Krankheit {0}\n",
      "a551:F:Heilmittel_beschaffen {1}\n",
      "a551:F:Anregen_zu_Beauftragungen {1}\n",
      "a551:h:Heilmittel_beschaffen {2}\n",
      "a551:f:Anlocken {2}\n",
      "a551:F:Werben_um_den_Auftrag {2}\n",
      "a551:Hh:Überweisung_an_anderen_Stifter_oder_Helfer {2}\n",
      "a551:F:Ausrüsten {2}\n",
      "a551:H:Gastgeberschaft_sichern {2}\n",
      "a551:H:Reittier_erhalten {3}\n",
      "a551:H:Anlocken_nachgehen {3}\n",
      "a551:Hh:Ausrüsten {3}\n",
      "a551:f:Festnahme_bzw_Sperren {3}\n",
      "a551:H:Identität_testen {3}\n",
      "a551:H:Versprechen_von_Hilfe_erhalten {3}\n",
      "a551:H:Flug_zeugendes_Transportmittel_erhalten {3}\n",
      "a551:Ff:Handlungsanweisung_Wegweisung_mit_Optionen {3}\n",
      "a551:h:Anlocken_nachgehen {3}\n",
      "a551:Hh:Wegweisung_mit_Optionen_nachgehen {4}\n",
      "a551:F:Gastgeberschaft_sichern {4}\n",
      "a551:H:Zurückgreifen {4}\n",
      "a551:h:Respektlosigkeit {4}\n",
      "a551:h:Sich_abseilen_lassen {4}\n",
      "a551:h:Festnahme_bzw_Sperren {4}\n",
      "a551:f:Missetat_planen {4}\n",
      "a551:H:Besitzer_des_Objektes_ergreifen {4}\n",
      "a551:H:Kontakt_wiederherstellen {4}\n",
      "a551:H:Heilmittel_ergreifen {5}\n",
      "a551:h:Missetat_Aneignen {5}\n",
      "a551:F:Zurückgreifen {5}\n",
      "a551:H:Vergütung_Gegenleistung_um_Hilfe {5}\n",
      "a551:F:Seltenen_Gegenstand_beschaffen {5}\n",
      "a551:H:Bestrafung {5}\n",
      "a551:F:Markieren {5}\n",
      "a551:H:Heiraten {5}\n",
      "a551:f:Zum_fremden_Ort_gelangen {5}\n",
      "a551:F:Kontakt_wiederherstellen {5}\n",
      "a551:h:Kontakt_abbrechen {6}\n",
      "a551:H:Zaubermittel_erhalten {6}\n",
      "a551:h:Identität_testen {6}\n",
      "a551:Hh:Heilmittel_beschaffen {6}\n",
      "End-a551:H:Bestrafung {6}\n",
      "End-a551:H:Heiraten {6}\n",
      "a551:h:Bestrafung {7}\n",
      "a551:H:Markieren_erkennen {7}\n",
      "a551:Hh:Vergeltung_Angst_um_Angst {7}\n",
      "a551:Hh:Krankheit_beheben {7}\n",
      "a551:F:Flucht {8}\n",
      "a551:H:Verfolgung_hindern {9}\n",
      "a551:F:Verfolgung_hindern {9}\n"
     ]
    }
   ],
   "source": [
    "sia_a=[]\n",
    "with open('output_1.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f) \n",
    "    for line in reader:\n",
    "            sia_a.append(line)\n",
    "sia_b=[]         \n",
    "with open('output_2.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f) \n",
    "    for line in reader:\n",
    "            sia_b.append(line)\n",
    "\n",
    "# Liste c\n",
    "sia_c=[]\n",
    "\n",
    "# Iteriere durch Liste a\n",
    "for element_a in sia_a:\n",
    "    # Extrahiere den linken String aus dem ersten Element in Liste a\n",
    "    left_string_a = element_a[0].split(' ')[0]\n",
    "\n",
    "    # Überprüfe, ob der linke String aus Liste a in Liste b vorhanden ist\n",
    "    #for element_b in sia_b:\n",
    "    if any(left_string_a in element_b[0] for element_b in sia_b):\n",
    "        # Wenn ja, füge das aktuelle Element aus Liste a zu Liste c hinzu\n",
    "        sia_c.append(element_a[0])\n",
    "\n",
    "# Ausgabe der generierten Liste c\n",
    "for element_c in sia_c:\n",
    "    print(element_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Typ: a551\n",
      " Motiv: a551:h:Krankheit\n",
      " Häufigkeit:2\n",
      "nx.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"900px\"\n",
       "            src=\"nx.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x228da505890>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.from_nx(G, )\n",
    "print(f' Typ: {mZahl}\\n Motiv: {mWert}\\n Häufigkeit:{haeufigkeit}')\n",
    "nt.show('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
